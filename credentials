import trino
import pandas as pd
try:
    # Set up Trino connection details
    conn = trino.dbapi.connect(
        host='172.23.56.100',
        port=8445,
        user='23127803',
        catalog='hive',
        schema='canvas',
        http_scheme='https',
        auth=trino.auth.BasicAuthentication("23127803", "Standups#23!"),
        verify=r"D:\jks\presto.com_cert_0.pem"
    )
    print("✅ Connected to Trino database successfully!")
 
    # Your query
    query = """
        select * from hive.canvas.subscriberdimension where indeactivationtime is null and date(recactivationtime) =date'2025-12-07'
    """
 
    # Run query
    cursor = conn.cursor()
    cursor.execute(query)
    rows = cursor.fetchall()
    # Display output
    # for row in rows:
    #     print(row)
    columns = [col[0] for col in cursor.description]
    # Convert to DataFrame
    df = pd.DataFrame(rows,columns=columns)
except Exception as e:
    print(f"❌ Error occurred: {e}")
cursor.close()



iceberg.adhoc.ufaa_23203159


so let's brainstorm first, this is what i now want, I need to create new tables, with new columns soo we start with the iceberg.adhoc.ufaadata2025  and then hive.sre.ufaadata2025. So for these two, we are adding the columns that is owner_name	owner_dob	owner_id	transaction_date	transaction_time	owner_due_amount	owner_msisdn	owner_code
sowe now have to create new tables for the letters that is for iceberg.adhoc.ufaaletters and hive.sre.ufaaletters,soo its columns being letter_sent	letter_date	letter_ref_no	owner_code, soo something connecting the two tables will be that the owner_code, I want every entry to have a unque code, that is you willhave to give me this, where every data run into thesystem gets the unique_code. So when everyentry has a unique code, then when one wants to assign the letter sent, they will just enter the unique code, and then it will be populating, because a letter can only be sent once, but you can also help me brainstorm on the best way to do this. Soo telll me what you think, dont start editting, and all this will be in the data version of our project.  
/



INSERT INTO iceberg.adhoc.ufaa_23203159
SELECT *
FROM hive.sre.UFAA_23203159;
